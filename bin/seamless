#!/usr/bin/env -S python3 -u
# type: ignore   # disable PyLance, as it cannot import seamless from here. Pylint still works correctly
# pylint: disable=import-self, wrong-import-position

"""Main command-line seamless executable script"""

__version__ = "0.14"

import argparse
from copy import copy
import sys
import os
import json
import traceback
import threading
import logging
import functools
import time
import pathlib

os.environ["__SEAMLESS_FRUGAL"] = "1"
import seamless

from seamless import Checksum, Buffer
from seamless.checksum.buffer_cache import buffer_cache
from seamless.util import unchecksum
from seamless.cmd.message import set_verbosity, message as msg, message_and_exit as err
from seamless.cmd import parsing
from seamless.cmd.parsing import get_commands, guess_arguments
from seamless.cmd.file_mapping import get_file_mapping
from seamless.cmd.file_load import files_to_checksums
from seamless.cmd.bash_transformation import (
    prepare_bash_transformation,
    run_transformation,
)
from seamless.cmd import interface
from seamless.cmd.exceptions import SeamlessSystemExit
from seamless.cmd.bytes2human import human2bytes
from seamless.cmd.get_results import get_results, get_result_buffer, maintain_futures
from seamless.Environment import Environment
from seamless.checksum.json import json_dumps
from seamless.workflow.core.direct.run import extract_dunder, tf_get_buffer
from seamless.workflow.core.transformation import SeamlessTransformationError

parser = argparse.ArgumentParser()
parser.add_argument(
    "-v",
    dest="verbosity",
    help="""Verbose mode.
Multiple -v options increase the verbosity. The maximum is 3""",
    action="count",
    default=0,
)
parser.add_argument(
    "-q", dest="verbosity", help="Quiet mode", action="store_const", const=-1
)

parser.add_argument(
    "-g1",
    help="""Disable argtyping guess rule 1.
This rule states that any argument with a file extension must exist as a file.""",
    action="store_true",
)

parser.add_argument(
    "-g2",
    help="""Disable argtyping guess rule 2.
This rule states that any argument without file extension must not exist as a file.""",
    action="store_true",
)

parser.add_argument(
    "-w",
    help="""Set the argtyping working directory to DIR.
This allows any file in DIR to be specified as argument.""",
    dest="workdir",
)

parser.add_argument(
    "-W",
    help="""Set the argtyping working directory to /.
This allows any file to be specified as argument.""",
    dest="workdir",
    action="store_const",
    const="/",
)

parser.add_argument(
    "-ms",
    help="""Set the argtyping file mapping mode to 'strip'.
Strip directory names. After stripping, all files must be unique.""",
    dest="file_mapping_mode",
    action="store_const",
    const="strip",
)


parser.add_argument(
    "-mx",
    help="""Set the argtyping file mapping mode to 'extension'.
Keep only file extensions. Rename the file name body to file1, file2, ...
Example: 
  python script.py data.txt 
  => 
  python file1.py file2.txt

Directories remain unchanged.""",
    dest="file_mapping_mode",
    action="store_const",
    const="extension",
)

parser.add_argument(
    "-c", help="Unquote the command line", dest="unquote", action="store_true"
)

parser.add_argument(
    "-cp",
    "--capture",
    help="""Add a file or directory to the result capture.

After the command line has completed, this file (or directory) will checksummed,
 added to the result, and (potentially) downloaded.
You can use ":" if the file is named differently on the server 
than the download target file. For example, "-cp foo:bar" will download
the result file "foo" on the server to the local file "bar".
An empty server file name captures stdout, e.g. "-cp :bar"
""",
    action="append",
    dest="extra_results",
)

parser.add_argument(
    "-i",
    "--input",
    help="""Add a file or directory as an input.

Normally, only the first command gets analyzed for input files and directories.
With this option, you can add an input manually.
""",
    action="append",
    dest="extra_inputs",
)

parser.add_argument(
    "-y",
    "--yes",
    dest="auto_confirm",
    help="""Sets any confirmation values to 'yes' automatically. Users will not be asked to confirm any file upload or download.
Uploads will happen without confirmation for up to 400 files and up to 100 MB in total.
Downloads will happen without confirmation for up to 2000 files and up to 500 MB in total.
These thresholds can be controlled by the environment variables:
SEAMLESS_MAX_UPLOAD_FILES, SEAMLESS_MAX_UPLOAD_SIZE, SEAMLESS_MAX_DOWNLOAD_FILES, SEAMLESS_MAX_DOWNLOAD_SIZE.""",
    action="store_const",
    const="yes",
)

parser.add_argument(
    "-n",
    "--no",
    dest="auto_confirm",
    help="""Sets any confirmation values to 'no' automatically. Users will not be asked to confirm any file upload or download.
Uploads will happen without confirmation for up to 400 files and up to 100 MB in total.
Downloads will happen without confirmation for up to 2000 files and up to 500 MB in total.
These thresholds can be controlled by the environment variables:
SEAMLESS_MAX_UPLOAD_FILES, SEAMLESS_MAX_UPLOAD_SIZE, SEAMLESS_MAX_DOWNLOAD_FILES, SEAMLESS_MAX_DOWNLOAD_SIZE.""",
    action="store_const",
    const="no",
)

parser.add_argument(
    "-nd",
    "--no-download",
    dest="no_download",
    help="Do not download any result files or directories, only their checksums",
    action="store_true",
)

parser.add_argument(
    "--scratch",
    help="Don't store any result values, only store the overall result checksum",
    default=False,
    action="store_true",
)

parser.add_argument(
    "--docker",
    "--docker-image",
    dest="docker_image",
    help="Specify Docker image where transformation is to run in",
)

parser.add_argument(
    "--conda",
    "--conda-env",
    "--conda-environment",
    dest="conda_environment",
    help="Specify an existing conda environment where the transformation could run in.",
)

parser.add_argument(
    "--ncores", help="Number of cores required. -1 means all available cores", type=int
)

parser.add_argument(
    "--fingertip",
    help="If the result checksum is known but the result unavailable: force evaluation",
    default=False,
    action="store_true",
)

parser.add_argument("--undo", help="Undo this seamless command", action="store_true")

parser.add_argument(
    "--direct-print",
    dest="direct_print",
    help="Attempt to print out stderr messages directly in the executor log",
    action="store_true",
)

parser.add_argument(
    "--local",
    help="""Don't delegate the transformation to the assistant, but execute directly.
This is most useful for scripts that contain themselves /bin/seamless commands""",
    action="store_true",
)

parser.add_argument(
    "--dry",
    "--dry-run",
    dest="dry_run",
    help="Only parse the arguments and prepare a transformation job, do not execute it",
    action="store_true",
    default=False,
)

parser.add_argument(
    "-j",
    "--write-job",
    dest="write_job",
    help="Requires --dry. Write out the (small) transformation job buffers into a job directory for seamless-run-transformation",
)

parser.add_argument(
    "--upload",
    help="Requires --dry. Upload input files and the (small) transformation job buffers",
    action="store_true",
)

parser.add_argument(
    "--qsubmit",
    "--qsub",
    help="Parse the arguments and write the job to a queue file",
    action="store_true",
    default=False,
)


parser.add_argument("command", nargs=argparse.REMAINDER)

args = parser.parse_args()

if args.dry_run:
    if args.qsubmit:
        err("--qsubmit and --dry cannot be used together")
else:
    if args.write_job:
        err("-j/--write-job requires --dry")
    if args.upload:
        err("--upload requires --dry")
if args.write_job:
    if os.path.exists(args.write_job):
        err(f"Output job directory '{args.write_job}' already exists")
command = args.command

verbosity = min(args.verbosity, 3)
set_verbosity(verbosity)
msg(1, "Verbosity set to {}".format(verbosity))
msg(1, "seamless {}".format(__version__))
msg(3, "Command:", json.dumps(command, indent=4))

if len(command) == 0:
    parser.print_usage()  # TODO: add to usage message
    sys.exit(1)

################################################################

workdir = os.getcwd()
if args.workdir is not None:
    if os.environ.get("DOCKER_IMAGE"):
        err(
            """DOCKER_IMAGE environment variable has been set.
We are running from inside a Docker container.
Setting workdir is not supported."""
        )
    workdir = os.path.expanduser(args.workdir)
    msg(1, "set argtyping working directory to: {}".format(workdir))

if args.qsubmit:
    queue_file = os.environ.get("SEAMLESS_QUEUE_FILE")
    if queue_file is None:
        queue_file = ".seamless-queue"
        msg(1, f"SEAMLESS_QUEUE_FILE not defined. Set queue file to '{queue_file}'")
    else:
        msg(1, f"Read queue file from SEAMLESS_QUEUE_FILE: '{queue_file}'")

################################################################

meta = None
if args.direct_print:
    if meta is None:
        meta = {}
    meta["__direct_print__"] = True

if args.ncores:
    if meta is None:
        meta = {}
    meta["ncores"] = args.ncores

################################################################

if args.unquote:
    if len(command) != 1:
        err(-1, "Unquote requires a single argument")
    commandstring = command[0]
else:
    commandstring = " ".join(command)

commands, first_pipeline, pipeline_redirect = get_commands(commandstring)

first_command = commands[0]

(
    interface_argindex,
    interface_file,
    interface_py_file,
    mapped_execarg,
) = interface.locate_files(first_command.words)

msg(1, f"First command: {first_command.commandstring}")

first_command_words = copy(first_command.words)
if mapped_execarg:
    first_command_words[0] = mapped_execarg
first_pipeline_words = copy(first_command_words)

argtypes_initial, results_initial = interface.get_argtypes_and_results(
    interface_file,
    interface_py_file,
    interface_argindex,
    first_command_words,
    first_command.words[0],
)

if interface_py_file is None:
    msg(2, "Try to obtain argtypes from rules")

    overrule_ext, overrule_no_ext = False, False
    if args.g1:
        msg(1, "disable argtyping guess rule 1")
        overrule_ext = True
    if args.g2:
        msg(1, "disable argtyping guess rule 2")
        overrule_no_ext = True

    try:
        if first_pipeline:
            for com in commands[1:first_pipeline]:
                first_pipeline_words += com.words
        argtypes_guess = guess_arguments(
            first_pipeline_words,
            overrule_ext=overrule_ext,
            overrule_no_ext=overrule_no_ext,
        )
    except ValueError as exc:
        err(*exc.args)
    if argtypes_initial is None:
        argtypes_initial = argtypes_guess
    else:
        argtypes_temp = argtypes_initial
        argtypes_initial = {}
        argtypes_initial.update(argtypes_guess)
        argtypes_initial.update(argtypes_temp)

argtypes_extra_inputs = {}
if args.extra_inputs:
    argtypes_extra_inputs = guess_arguments(
        args.extra_inputs,
        overrule_ext=True,
        overrule_no_ext=True,
    )
    for inp in args.extra_inputs:
        assert inp in argtypes_extra_inputs
        if (
            argtypes_extra_inputs[inp] == "value"
            or argtypes_extra_inputs[inp]["type"] == "value"
        ):
            err(f"--input argument '{inp}': file/directory does not exist")

argtypesstr = json.dumps(unchecksum(argtypes_initial), sort_keys=False, indent=2)
msg(1, "initial argtypes dict:\n" + argtypesstr)

file_mapping_mode = args.file_mapping_mode
if file_mapping_mode is None:
    file_mapping_mode = "literal"

try:
    argtypes = get_file_mapping(
        argtypes_initial, mapping_mode=file_mapping_mode, working_directory=workdir
    )
    argtypes_extra_inputs2 = get_file_mapping(
        argtypes_extra_inputs, mapping_mode=file_mapping_mode, working_directory=workdir
    )
    argtypes_extra_inputs2.pop("@order")
    argtypes.update(argtypes_extra_inputs2)
except ValueError as exc:
    err(*exc.args)

argtypes = unchecksum(argtypes)
argtypesstr = json.dumps(argtypes, sort_keys=False, indent=2)
msg(1, "argtypes dict:\n" + argtypesstr)

mapped_first_pipeline = argtypes["@order"]

################################################################
assert len(mapped_first_pipeline) == len(first_pipeline_words)
word_substitutions = {}
for wordnr, word in enumerate(mapped_first_pipeline):
    offset = 0
    for com in commands:
        if wordnr >= len(com.words) + offset:
            offset += len(com.words)
            continue
        else:
            break
    old_word = com.words[wordnr - offset]
    if word != old_word:
        node = com.wordnodes[wordnr - offset]
        word_substitutions[node] = word

################################################################
interface_data = {}
make_executables = []


def update_interface_data(new_interface_data, first):
    changed = False
    for k, v in new_interface_data.items():
        if k in ("argtypes", "files", "directories", "shim"):
            continue
        curr = interface_data.get(k)
        if k == "results":
            if first:
                continue
            if isinstance(v, list):
                v = {kk: None for kk in v}
            if k not in interface_data:
                interface_data[k] = {}
            if v:
                interface_data[k].update(v)
                changed = True
            continue
        if k == "environment":
            if curr is None:
                interface_data[k] = v
                continue
            if curr == v:
                continue
            raise NotImplementedError("Multiple environments")

        changed = True
        if curr is None:
            interface_data[k] = v
        else:
            if isinstance(v, list) != isinstance(curr, list):
                interface_data[k] = v
            elif isinstance(curr, list):
                interface_data[k] += v
            else:
                interface_data[k] = v
    if changed:
        msg(3, f"Updated interface data: {json.dumps(interface_data, indent=2)}")


for commandnr, command in enumerate(commands):
    if commandnr > 0:
        msg(1, f"Command #{commandnr+1}: {command.commandstring}")
        (
            interface_argindex,
            interface_file,
            interface_py_file,
            mapped_execarg,
        ) = interface.locate_files(command.words)
    if interface_file is not None or interface_py_file is not None:
        if commandnr > 0:
            msg(
                1,
                f"Interface files found for command #{commandnr+1}: {command.commandstring}",
            )

    shim = None
    if interface_file is not None:
        msg(2, f"loading {interface_file}")
        _new_interface_data = interface.load(interface_file.as_posix())
        msg(3, f"{interface_file} content: {json.dumps(_new_interface_data, indent=2)}")
        new_shim = _new_interface_data.get("shim")
        if new_shim is not None:
            shim = new_shim
        update_interface_data(_new_interface_data, first=(commandnr == 0))

    if commandnr == 0:
        interface_py_file = None
    if interface_py_file is not None:
        if commandnr == 0:
            command_words = mapped_first_pipeline
        else:
            command_words = copy(command.words)
            if mapped_execarg:
                command_words[0] = mapped_execarg

        arguments = command_words[interface_argindex + 1 :]
        _new_interface_data = interface.interface_from_py_file(
            interface_py_file, arguments
        )
        msg(
            3,
            f"{interface_py_file} result: {json.dumps(_new_interface_data, indent=2)}",
        )
        new_shim = _new_interface_data.get("shim")
        if new_shim is not None:
            shim = new_shim
        update_interface_data(_new_interface_data, first=(commandnr == 0))

        if shim:
            wordnode = command.wordnodes[interface_argindex]
            word_substitutions[wordnode] = shim

    if mapped_execarg and not shim:
        wordnode = command.wordnodes[0]
        word = command.words[0]
        word = word_substitutions.get(wordnode, word)
        if word not in ("cd", "echo", "eval", "exec", "export", "set", "source"):
            make_executables.append(word)

################################################################

redirection = None
if pipeline_redirect:
    redirection_node = pipeline_redirect
else:
    redirection_node = parsing.get_redirection(commands[-1])
if redirection_node is not None:
    redirection = redirection_node.word
    redirection = os.path.expanduser(redirection)
    if workdir is not None and redirection.startswith(workdir):
        redirection = redirection[len(workdir) :]
    if redirection != redirection_node.word:
        word_substitutions[redirection_node] = redirection

if results_initial is None:
    results_initial = {}
else:
    msg(2, f"Initial result targets from first command: {results_initial}")
    decal = os.path.relpath(os.getcwd(), workdir)
    if decal != ".":
        results_initial2 = results_initial.copy()
        for _k, _v in results_initial.items():
            if _v is None:
                _kk = os.path.join(decal, _k)
                results_initial2.pop(_k)
                results_initial2[_kk] = _v
        results_initial = results_initial2

results = results_initial.copy()
results.update(interface_data.get("results", {}))

if args.extra_results:
    for extra_result in args.extra_results:
        pos = extra_result.find(":")
        if pos == -1:
            results[extra_result] = None
        elif pos == 0:
            results["STDOUT"] = extra_result[1:]
        else:
            results[extra_result[:pos]] = extra_result[pos + 1 :]

if not results and not redirection:
    if args.no_download or args.scratch:
        err("Download disabled. Refuse to print output to screen.")
    elif args.qsubmit:
        err(
            "Submit to a queue, but no capture/download targets. Refuse to print output to screen."
        )
    capture_stdout = True
    result_targets = None
else:
    if "STDOUT" in results:
        capture_stdout = True
    else:
        capture_stdout = False
    result_targets = results.copy()
    if redirection:
        result_targets[redirection] = None
    msg(1, f"Identify result targets: {result_targets}")

################################################################

env = Environment()

docker_image = args.docker_image
if docker_image is None:
    docker_image = interface_data.get("environment", {}).get("docker_image")

if docker_image is not None:
    msg(1, f'Set Docker image to "{docker_image}"')
    env.set_docker({"name": docker_image})

conda_environment = args.conda_environment
if conda_environment is None:
    conda_environment = interface_data.get("environment", {}).get("conda_environment")

if conda_environment is not None:
    msg(1, f'Set conda environment to "{conda_environment}"')
    env.set_conda_env(conda_environment)

if args.dry_run or args.qsubmit:
    if args.upload or args.qsubmit:
        if seamless.delegate(level=2):
            sys.exit(1)
    else:
        seamless.delegate(level=1)
else:
    if args.local:
        if seamless.delegate(level=3):
            sys.exit(1)
    else:
        if seamless.delegate():
            sys.exit(1)

# TODO: max_upload_files, max_upload_size:
# TODO: max_download_files, max_download_size:

#   modify the assistant protocol so that the assistant can provide it.

max_upload_files = os.environ.get("SEAMLESS_MAX_UPLOAD_FILES", "400")
max_upload_files = int(max_upload_files)
max_upload_size = os.environ.get("SEAMLESS_MAX_UPLOAD_SIZE", "100 MB")
max_upload_size = human2bytes(max_upload_size)

max_download_files = os.environ.get("SEAMLESS_MAX_DOWNLOAD_FILES", "2000")
max_download_files = int(max_download_files)
max_download_size = os.environ.get("SEAMLESS_MAX_DOWNLOAD_SIZE", "500 MB")
max_download_size = human2bytes(max_download_size)

paths = set()
directories = {}
mapping = {}
direct_checksums = {}
direct_checksum_directories = {}
for argname, arg in argtypes.items():
    direct_checksum = None
    if isinstance(arg, dict):
        argtype = arg.get("type")
        path = arg.get("mapping", argname)
        direct_checksum = arg.get("checksum")
    else:
        argtype = arg
        path = argname
    if argtype in ("file", "directory"):
        if direct_checksum:
            if argtype == "directory":
                direct_checksum_directories[argname] = argname
            direct_checksums[argname] = direct_checksum
        else:
            paths.add(path)
            if argtype == "directory":
                directories[path] = argname
            mapping[argname] = path

try:
    file_checksum_dict, _ = files_to_checksums(
        paths,
        max_upload_size=max_upload_size,
        max_upload_files=max_upload_files,
        directories=directories,
        auto_confirm=args.auto_confirm,
        dry_run=(args.dry_run and not args.upload),
    )
except SeamlessSystemExit as exc:
    err(*exc.args)

checksum_dict = {k: file_checksum_dict[v] for k, v in mapping.items()}
checksum_dict.update(direct_checksums)
checksum_dictstr = json.dumps(checksum_dict, sort_keys=True, indent=2)
msg(2, "file/directory checksum dict:\n" + checksum_dictstr)
directories.update(direct_checksum_directories)
if len(directories):
    msg(2, "directories:", directories)

################################################################

for node in sorted(word_substitutions.keys(), key=lambda node: -node.pos[0]):
    word = word_substitutions[node]
    commandstring = commandstring[: node.pos[0]] + word + commandstring[node.pos[1] :]

msg(1, "bash command:\n", commandstring, "\n")

################################################################

command = commandstring.split()
variables = None
if len(commands) == 1:
    canonical = interface_data.get("canonical", [])
    for canon in canonical:
        vars = canon.get("variables", [])
        varnames = [var["name"] for var in vars]
        vartypes = {var["name"]: var["celltype"] for var in vars}
        canon_cmd = canon["command"].split()
        if len(canon_cmd) != len(command):
            continue
        for w1, w2 in zip(canon_cmd, command):
            if w1 == w2:
                continue
            if w1[0] == "$" and w1[1:] in varnames:
                continue
            break
        else:
            msg(1, f"Found canonical command match:\n  {canon['command']}")
            variables = {}
            for w1, w2 in zip(canon_cmd, command):
                if w1[0] == "$" and w1[1:] in varnames:
                    varname = w1[1:]
                    variables[varname] = (w2, vartypes[varname])
            msg(2, f"Variables:\n  {variables}")
            commandstring = canon["command"]

################################################################

transformation_checksum, transformation_dict = prepare_bash_transformation(
    commandstring,
    checksum_dict,
    directories=list(directories.values()),
    make_executables=make_executables,
    capture_stdout=capture_stdout,
    result_targets=result_targets,
    environment=env._to_lowlevel(bash=True),
    variables=variables,
    meta=meta,
    dry_run=(args.dry_run and not args.upload),
)

if make_executables:
    msg(
        2,
        "The following files will be made executable inside the transformer\n:\n  {}".format(
            "\n  ".join(make_executables)
        ),
    )
else:
    msg(3, "No files will be made executable inside the transformer")

################################################################

transformation_checksum = Checksum(transformation_checksum).hex()
msg(3, f"Transformation checksum: {transformation_checksum}")


def file_write(handle, buf):
    if isinstance(buf, Buffer):
        buf = buf.value
    handle.write(buf)


if args.dry_run:
    if args.write_job is not None:
        os.mkdir(args.write_job)
        transformation_buffer = tf_get_buffer(transformation_dict)
        dunder = extract_dunder(transformation_dict)
        if dunder:
            with open(os.path.join(args.write_job, "dunder.json"), "wb") as _f:
                _f.write(json_dumps(dunder, as_bytes=True) + b"\n")
        code_checksum = transformation_dict["code"][2]
        env_checksum = transformation_dict.get("__env__")
        if args.upload:
            with open(
                os.path.join(args.write_job, "transformation.json.CHECKSUM"), "w"
            ) as _f:
                _f.write(transformation_checksum)
        else:
            with open(os.path.join(args.write_job, "transformation.json"), "wb") as _f:
                file_write(_f, transformation_buffer)
            command = buffer_cache.buffer_cache[bytes.fromhex(code_checksum)]
            with open(os.path.join(args.write_job, "command.txt"), "wb") as _f:
                file_write(_f, command)
            if env_checksum is not None:
                env_buffer = buffer_cache.buffer_cache[bytes.fromhex(env_checksum)]
                with open(os.path.join(args.write_job, "env.json"), "wb") as _f:
                    file_write(_f, env_buffer)

    if args.upload or args.write_job:
        print(transformation_checksum)

        # kludge to hide spurious "buffers undestroyed" warnings

        logger = logging.getLogger("seamless")
        logger.setLevel(logging.ERROR)

    sys.exit(0)

elif args.qsubmit:
    if args.auto_confirm not in ("yes", "no"):
        msg(
            1,
            "qsubmit: download confirmation cannot be interactive. --yes not present => default to --no.",
        )
        auto_confirm = "no"
    else:
        auto_confirm = args.auto_confirm

    assert result_targets

    original_command = "seamless " + " ".join(sys.argv[1:])
    params = {
        "undo": args.undo,
        "scratch": args.scratch,
        "workdir": workdir,
        "download": not args.no_download,
        "auto_confirm": auto_confirm,
        "capture_stdout": capture_stdout,
        "max_download_size": max_download_size,
        "max_download_files": max_download_files,
    }
    queue_command = {
        "queue_command": "SUBMIT",
        "original_command": original_command,
        "transformation_checksum": str(transformation_checksum),
        "transformation_dict": transformation_dict,
        "result_targets": result_targets,
        "params": params,
    }
    qcj = json.dumps(queue_command, indent=2)

    lockpath = queue_file + ".LOCK"

    while 1:
        try:
            lock_stat_result = os.stat(lockpath)
        except FileNotFoundError:
            break
        lock_mtime = lock_stat_result.st_mtime
        if time.time() - lock_mtime > 30:
            msg(1, f"Stale lock on {queue_file}, breaking it...")
            break
        time.sleep(1)

    try:
        pathlib.Path(lockpath).touch()
        with open(queue_file, "ab") as fp:
            fp.write(qcj.encode() + b"\x00")

    finally:
        try:
            pathlib.Path(lockpath).unlink()
        except FileNotFoundError:
            pass

    # kludge to hide spurious "buffers undestroyed" warnings

    logger = logging.getLogger("seamless")
    logger.setLevel(logging.ERROR)

    msg(1, "Command submitted to queue file")

    sys.exit(0)

delete_futures_event = threading.Event()

maintain_fut = functools.partial(
    maintain_futures,
    workdir,
    transformation_checksum,
    result_targets,
    delete_futures_event=delete_futures_event,
    msg_func=msg,
)

if result_targets:
    maintain_futures_thread = threading.Thread(
        target=maintain_fut, name=maintain_futures
    )
    maintain_futures_thread.start()

try:
    result_fingertip = True
    if args.no_download:
        result_fingertip = False
    if result_targets:
        result_fingertip = True
    result_checksum = run_transformation(
        transformation_dict,
        undo=args.undo,
        fingertip=result_fingertip,
        scratch=args.scratch,
    )
except SeamlessTransformationError as exc:
    traceback.print_exc(limit=0)
    sys.exit(1)
finally:
    delete_futures_event.set()
################################################################

if not args.undo:
    msg(1, "Transformation finished")
    msg(2, "Result checksum: {}".format(result_checksum))


# kludge to hide spurious "buffers undestroyed" warnings
logger = logging.getLogger("seamless")
logger.setLevel(logging.ERROR)


if args.undo:
    if result_checksum is None:
        exit(1)
    else:
        msg(1, "Transformation undone")
        exit(0)

else:
    result_buffer = get_result_buffer(
        result_checksum,
        do_fingertip=args.fingertip,
        do_scratch=args.scratch,
        has_result_targets=(True if result_targets else False),
        err_func=err,
    )
    result = get_results(
        result_targets,
        result_checksum,
        result_buffer,
        workdir=workdir,
        do_scratch=args.scratch,
        do_download=not args.no_download,
        do_capture_stdout=capture_stdout,
        max_download_size=max_download_size,
        max_download_files=max_download_files,
        do_auto_confirm=args.auto_confirm,
        msg_func=msg,
    )
    if result is not None:
        print(result)
