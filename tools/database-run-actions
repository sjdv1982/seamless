#!/usr/bin/env python3

import os
import json
import cson
import csv
from ruamel.yaml import YAML
yaml = YAML(typ='safe')
import time
import re
import shutil
import subprocess
import multiprocessing
import concurrent.futures
import copy
import gc

from seamless import calculate_checksum
from silk.mixed import MAGIC_SEAMLESS_MIXED, MAGIC_NUMPY
from seamless.core.cache.buffer_cache import buffer_cache
from seamless.core.convert import try_convert
from seamless.core.buffer_info import BufferInfo, convert_from_buffer_info
from seamless.core.protocol.serialize import serialize_sync as serialize
from seamless.core.protocol.deserialize import deserialize_sync as deserialize
from seamless.core.protocol.calculate_checksum import calculate_checksum_cache, checksum_cache

calculate_checksum_cache.disable()
checksum_cache.disable()
from seamless.core.protocol.serialize import serialize_cache
serialize_cache.disable()
from seamless.core.protocol.deserialize import deserialize_cache
deserialize_cache.disable()

def err(*args, **kwargs):
    print("ERROR: " + args[0], *args[1:], **kwargs)
    exit(1)

def report(*args_, **kwargs):
    if not args.verbose:
        return
    print(*args_, **kwargs)

def write_csv(d:dict, filename):
    with open(filename, 'w', newline='') as csvfile:
        csvwriter = csv.writer(csvfile, delimiter=' ',
                                quotechar='|', quoting=csv.QUOTE_MINIMAL)
        for key, value in d.items():
            if isinstance(value, list):
                csvwriter.writerow([key] + value)
            elif isinstance(value, tuple):
                csvwriter.writerow([key] + list(value))
            else:
                csvwriter.writerow([key, value])

def load_csv(filename):
    result = {}
    with open(filename, newline='') as csvfile:
        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')
        for row in spamreader:
            if len(row) == 2:
                result[row[0]] = row[1]
            else:
                result[row[0]] = row[1:]
    return result

def get_plain_buffer(d:dict):
    return serialize(d, "plain")

def write_json(d:dict, filename):
    with open(filename, "wb") as f:
        f.write(get_plain_buffer(d))

def remove_stale_buffer_hardlink(checksum, inode):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        buffer_inode = str(os.stat(buffer_file).st_ino)
        if buffer_inode == inode:
            report("Remove stale buffer hardlink {}".format(buffer_file))
            os.remove(buffer_file)

def intern_buffer(checksum, filename, hardlink):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        # We trust that the buffer is correct...
        return
    if hardlink:
        os.link(filename, buffer_file)
    else:
        shutil.copyfile(filename, buffer_file)

def get_buffer_info(checksum):
    d = buffer_info_bucket.get(checksum)
    if d is None:
        return None
    assert isinstance(d, dict), d
    return BufferInfo(checksum, d)

def set_buffer_info(buffer_info: BufferInfo):
    checksum = buffer_info.checksum.hex()
    d = buffer_info.as_dict()
    buffer_info_bucket.set(checksum, d)

def write_buffer_length(checksum, length):
    buffer_info = get_buffer_info(checksum)
    if buffer_info is None:
        buffer_info = BufferInfo(checksum)
    if buffer_info.length != length:
        buffer_info.length = length
        set_buffer_info(buffer_info)
    

def update_buffer_info(buffer_info:BufferInfo, attr, value):
    # Adapted from buffer_cache.update_buffer_info
    co_flags = {
        "is_json": ("is_utf8",),
        "json_type": ("is_json",),
        "is_json_numeric_array": ("is_json",),
        "is_json_numeric_scalar": ("is_json",),
        "dtype": ("is_numpy",),
        "shape": ("is_numpy",),
    }
    anti_flags = {
        "is_json": ("is_numpy", "is_seamless_mixed"),
        "is_numpy": ("is_json", "is_seamless_mixed"),
        "is_seamless_mixed": ("is_json", "is_numpy"),
    }

    #if attr == "is_numpy":
    #    assert value == False
    #if attr == "json_type":
    #    print("B!", buffer_info.as_dict())
    buffer_info[attr] = value
    if value:
        for f in co_flags.get(attr, []):
            update_buffer_info(buffer_info, f, True)
        for f in anti_flags.get(attr, []):
            update_buffer_info(buffer_info, f, False)
    elif value == False:
        for f in co_flags.get(attr, []):
            update_buffer_info(buffer_info, f, False)
    #if attr == "json_type":
    #    print("BB!", buffer_info.as_dict())
    #    exit(0)

def guarantee_buffer_info(buffer_info:BufferInfo, buffer, celltype:str):
    """Modify buffer_info to reflect that checksum is surely deserializable into celltype
    Adapted from buffer_cache.guarantee_buffer_info
    """
    if celltype == "bytes":
        return
    if celltype == "checksum":
        # out-of-scope for buffer info
        return
    if celltype in ("ipython", "python", "cson", "yaml"):
        # parsability as IPython/python/cson/yaml is out-of-scope for buffer info
        celltype = "text"
    if celltype == "mixed":
        if buffer is not None:
            if buffer.startswith(MAGIC_NUMPY):
                update_buffer_info(buffer_info, "is_numpy", True)
            elif buffer.startswith(MAGIC_SEAMLESS_MIXED):
                update_buffer_info(buffer_info, "is_seamless_mixed", True)
            else:
                update_buffer_info(buffer_info, "is_json", True)
    elif celltype == "binary":
        update_buffer_info(buffer_info, "is_numpy", True)
    elif celltype == "plain":
        update_buffer_info(buffer_info, "is_json", True)
    elif celltype == "text":
        update_buffer_info(buffer_info, "is_utf8", True)
    elif celltype in ("str", "int", "float", "bool"):
        update_buffer_info(buffer_info, "json_type", celltype)

def do_operation(cmd, source_celltype, target_celltype, ops):
    # TODO: 
    # For cmd, try to synthesize a TransformationJob:
    #  e.g. bash transformer "ln -s inp inp.gz && gunzip -c inp.gz"
    # re-use existing TransformationJob results from jobs/, and write them also!
    # TODO: for some reason, this causes massive memory leaks, 
    # even if all Seamless caches are cleaned and emptied.
    # For now, just clean up the executor every 1000 jobs
    result = []
    for checksum, filename in ops:
        if cmd is not None:
            proc = subprocess.run(cmd + [filename], stdout=subprocess.PIPE, text=False)
            if proc.returncode != 0:
                if proc.returncode == -2:
                    raise KeyboardInterrupt
                print("Error in operationing {}".format(filename))            
            buf = proc.stdout
        else:
            with open(filename, "rb") as f:
                buf = f.read()
        buf_checksum = calculate_checksum(buf, hex=True)
        if source_celltype != target_celltype:
            source_buf = buf
            source_checksum = buf_checksum

            old_source_buffer_info = get_buffer_info(source_checksum)
            source_buffer_info = copy.deepcopy(old_source_buffer_info)
            if source_buffer_info is None:
                source_buffer_info = BufferInfo(source_checksum)
                source_buffer_info.length = len(source_buf)
            buffer_cache.buffer_info[bytes.fromhex(source_checksum)] = source_buffer_info

            try:
                source_buf.decode()
                source_text = True
            except Exception:
                source_text = False
            update_buffer_info(source_buffer_info, "is_utf8", source_text)

            cs = bytes.fromhex(source_checksum)
            result_buf = None
            conversion_result = try_convert(
                cs, source_celltype, target_celltype,
                buffer_info=source_buffer_info,
                buffer=source_buf
            )
            if conversion_result == True:
                result_checksum = source_checksum
            elif isinstance(conversion_result, bytes):
                result_checksum = conversion_result.hex()
            else:
                value = deserialize(source_buf, bytes.fromhex(source_checksum), source_celltype, copy=False)                
                result_buf = serialize(value, target_celltype)
                result_checksum = calculate_checksum(result_buf, hex=True)
                
            if result_checksum != source_checksum:
                if result_buf is None:
                    result_buf = buffer_cache.get_buffer(result_checksum)
                assert result_buf is not None                

            old_result_buffer_info = get_buffer_info(result_checksum)
            result_buffer_info = copy.deepcopy(old_result_buffer_info)
            if result_buffer_info is None:
                result_buffer_info = BufferInfo(result_checksum)
                result_buffer_info.length = len(result_buf)
            result_buffer_info2 = buffer_cache.buffer_info.get(bytes.fromhex(result_checksum))
            if result_buffer_info2 is not None:
                result_buffer_info.update(result_buffer_info2)
                
            if source_buffer_info != old_source_buffer_info:
                set_buffer_info(source_buffer_info)

            if result_buffer_info != old_result_buffer_info:
                set_buffer_info(result_buffer_info)                    
        else:
            old_buffer_info = get_buffer_info(buf_checksum)
            buffer_info = copy.deepcopy(old_buffer_info)
            if buffer_info is None:
                buffer_info = BufferInfo(buf_checksum)
                buffer_info.length = len(buf)
            try:
                buf.decode()
                buf_text = True
            except Exception:
                buf_text = False
            update_buffer_info(buffer_info, "is_utf8", buf_text)
            result_buf = buf
            result_checksum = buf_checksum
            if buffer_info != old_buffer_info:
                set_buffer_info(buffer_info)
        
        buffer_file = os.path.join(buffer_dir, result_checksum)
        if os.path.exists(buffer_file):
            # We trust that the buffer is correct...
            pass
        else:
            assert result_buf is not None
            try:
                with open(buffer_file, "wb") as f:
                    f.write(result_buf)
            except Exception:
                os.remove(buffer_file)
        buffer_cache.uncache_buffer(result_checksum)
        result.append((checksum, result_checksum))
    gc.collect()
    return result

def write_deepcontent(deepdict_checksum, deepdict):    
    deep_indices_dir = os.path.join(SDB, "deep_indices")
    if not os.path.exists(deep_indices_dir):
        os.mkdir(deep_indices_dir)
    buffersizefile = os.path.join(SDB, "deep_indices", "buffersizes.csv")
    if os.path.exists(buffersizefile):
        buffersizes = load_csv(buffersizefile)
    else:
        buffersizes = {}
    deepcontent_size = 0
    new_checksums = []
    for checksum in deepdict.values():
        if checksum in buffersizes:
            deepcontent_size += int(buffersizes[checksum])
        else:
            new_checksums.append(checksum)
    for n, checksum in enumerate(new_checksums):
        if n > 1 and n % 5000 == 0:
            report("Calculating deep content size: {}/{}".format(
                n, len(new_checksums)
            ))
        buffer_info = get_buffer_info(checksum)
        try:
            if buffer_info is None:
                raise Exception(checksum)
            buffersize = buffer_info.length
            if buffersize is None:
                raise Exception(checksum, buffer_info.as_dict())
        except Exception:
            buffer_file = os.path.join(buffer_dir, checksum)
            with open(buffer_file, "rb") as f:
                buffer = f.read()
                buffersize = len(buffer)
                buffer_info = BufferInfo(checksum)
                update_buffer_info(buffer_info, "length", buffersize)
        deepcontent_size += buffersize
        buffersizes[checksum] = buffersize
    if len(new_checksums):
        write_csv(buffersizes, buffersizefile)        
    report("Write deep content size for {}: {}".format(deepdict_checksum, deepcontent_size))
    deepcontentfile = os.path.join(SDB, "deep_indices", "deepcontent.csv")
    if os.path.exists(deepcontentfile):
        deepcontent = load_csv(deepcontentfile)
    else:
        deepcontent = {}
    deepcontent[deepdict_checksum] = deepcontent_size
    write_csv(deepcontent, deepcontentfile)


def get_subdir(subdirname):
    subdir = os.path.join(SDB, subdirname)
    if not os.path.exists(subdir):
        os.mkdir(subdir)
        report("Created {}".format(subdir))
    if not os.path.isdir(subdir):
        err("{} is not a directory".format(subdir))
    return subdir

from database_bucket import TopBucket

import argparse
parser = argparse.ArgumentParser()
parser.add_argument(
    "database_dir",
    help="Seamless database directory to operate on",
    type=str
)
parser.add_argument(
    "actionfile",
    help="Database action file (json, cson or yaml) to run",
    type=argparse.FileType('r')
)
parser.add_argument(
    "--verbose", "-v",
    action="store_true"
)
parser.add_argument(
    "--force", "-f",
    action="store_true"
)

args = parser.parse_args()

SDB = args.database_dir
if not os.path.isdir(SDB):
    print("Seamless database directory {} does not exist".format(SDB))

buffer_info_dir = os.path.join(SDB, "buffer_info")
buffer_info_bucket = TopBucket(buffer_info_dir)

afname = args.actionfile.name
if afname.endswith(".json"):
    loader = json.load
elif afname.endswith(".cson"):
    loader = cson.load
elif afname.endswith(".yaml"):
    def loader(file):
        return yaml.load(file.read())
else:
    err("actionfile must be .json, .cson or .yaml")

try:
    actiondict = loader(args.actionfile)
except Exception as exc:
    err("could not parse actionfile")

report("Loaded actionfile:")
report(json.dumps(actiondict, indent=2))


# TODO: probably run actions against a JSON schema...
ok = False
if isinstance(actiondict, dict):
    while 1:
        if "directory" not in actiondict:
            break
        if "actions" not in actiondict:
            break
        ok = True
        break
if not ok:
    err("malformatted actionfile")
# /TODO

default_collection_name = actiondict["directory"]["collection"]
cross_device = actiondict["directory"]["cross_device"]
dirpath = actiondict["directory"]["path"]
if not os.path.exists(dirpath):
    err("{} does not exist".format(dirpath))
if not os.path.isdir(dirpath):
    err("{} is not a directory".format(dirpath))

# Build inode table. 
# If there is an old inode table:
#   remove interned buffers that point to an inode that has changed
inode_dir = get_subdir("inodes")
buffer_dir = get_subdir("buffers")
inode_table_file = os.path.join(inode_dir, default_collection_name) + ".csv"
try:
    old_inode_table = load_csv(inode_table_file)
except Exception as exc:
    old_inode_table = {}

inode_table = {}
inode_to_entry = {}
nfiles = 0
inodes_changed = False
last_inode_write = time.time()
for root, dirnames, filenames in os.walk(dirpath, followlinks=False):
    nfiles += len(filenames)
for root, dirnames, filenames in sorted(os.walk(dirpath, followlinks=False)):
    for filename in filenames:
        filepath = os.path.join(root, filename)
        if not root.startswith(dirpath): # would be weird...
            report("Error in {}".format(filepath))
            continue
        entry = os.path.join(root[len(dirpath):], filename).lstrip("/")
        try:
            inode = str(os.stat(filepath, follow_symlinks=False).st_ino)
            mtime = str(os.stat(filepath, follow_symlinks=False).st_mtime)
        except OSError:
            report("Error in {}".format(filepath))
            continue
        
        inode_to_entry[inode] = entry
        from_cache = False
        if inode in old_inode_table:
            v = old_inode_table[inode]
            if isinstance(v, list) and len(v) == 2:
                old_checksum, old_mtime = v
                if mtime == old_mtime:
                    inode_table[inode] = v
                    from_cache = True

        if not from_cache:            
            try:
                with open(filepath, "rb") as f:
                    content = f.read()
            except OSError:
                report("Error in {}".format(filepath))
                continue
            checksum = calculate_checksum(content, hex=True)
            write_buffer_length(checksum, len(content))
            if inode in old_inode_table:
                # If there is a buffer hardlink with the old checksum 
                #  that points to the inode, clean it up
                old_checksum, _ = old_inode_table[inode]
                remove_stale_buffer_hardlink(old_checksum, inode)
            inode_table[inode] = [checksum, mtime]
            inodes_changed = True

            if len(inode_table) % 1000 == 0:
                report("Calculated checksums for {}/{} files".format(
                    len(inode_table), nfiles
                ))
        elapsed_time = time.time()
        if elapsed_time - last_inode_write > 20:
            report("Save intermediate inode table to {}".format(inode_table_file)) 
            curr_inode_table = old_inode_table.copy()
            curr_inode_table.update(inode_table)
            write_csv(curr_inode_table, inode_table_file)
            last_inode_write = elapsed_time

if not inodes_changed:
    if args.force:
        report("Inode table unchanged, but execution is forced")
    else:
        print("""Inode table unchanged. 
Unless the action file was modified, there is nothing to do. 
Use --force to force execution""")
        exit(0)
else:
    report("Write inode table to {}".format(inode_table_file))
    write_csv(inode_table, inode_table_file)

'''
###
high_inodes = sorted(list(inode_table.keys()))[1000:]
for inode in high_inodes:
    inode_table.pop(inode)
    inode_to_entry.pop(inode)
###
'''

report("Running actions")
collections = {
    default_collection_name: {
        "name": default_collection_name,
        "default": True
    }
}
actions = actiondict.get("actions", [])

executor = concurrent.futures.ProcessPoolExecutor()
try:
    for action in actions:
        name = action["action"]
        
        title = action.get("collection")
        if title is None:
            title = action.get("source_collection", default_collection_name)
        elif "source_collection" in action:
            title = action["source_collection"] + " => " + title
        report('Action {} "{}"'.format(name, title))
        if name == "intern_collection":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]        
            hardlink = action["hardlink"]
            buffer_dir = get_subdir("buffers")
            if source_collection.get("default"):
                if hardlink and cross_device:
                    # TODO: do such checks before running any actions
                    err("Cross-device directory cannot be interned with hardlinks")
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    filename = os.path.join(dirpath, entry)
                    intern_buffer(checksum, filename, hardlink)
                    
            elif source_collection.get("copied"):
                raise NotImplementedError # TODO: make a test for this
            else:
                msg = 'Action "intern_collection": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))
            
            source_collection["interned"] = True

        elif name == "operation":
            nproc = max(int(multiprocessing.cpu_count()/2), 1)
            target_collection_name = action["collection"]
            assert target_collection_name not in collections, target_collection_name
            command = action.get("command")
            assert command in ("gunzip", "unzip", "bunzip2", None)
            if command == "gunzip":
                cmd = ["gunzip", "-c"]
            elif command == "bunzip2":
                cmd = ["bunzip2", "-c"]
            elif command == "unzip":
                cmd = ["unzip", "-p"]
            else:
                cmd = None
            target_celltype = action.get("celltype")
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]        
            buffer_dir = get_subdir("buffers")
            operation_dir = get_subdir("operations")
            operation_file = os.path.join(operation_dir, target_collection_name) + ".csv"
            if os.path.exists(operation_file):
                old_operation = load_csv(operation_file)
            else:
                old_operation = {}

            operation = {}
            ops = []
            noperation = 0
            if source_collection.get("default"):
                origin_collection_name = default_collection_name
                origin_collection = collections[default_collection_name]
                source_celltype = "bytes"
                done_checksums = set()
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    if checksum in old_operation:
                        result_checksum = old_operation[checksum]
                        result_checksum_file = os.path.join(buffer_dir, result_checksum)
                        if os.path.exists(result_checksum_file):
                            noperation += 1
                            operation[checksum] = result_checksum
                            continue                
                    if checksum in done_checksums:
                        continue
                    noperation += 1                
                    done_checksums.add(checksum)
                    entry = inode_to_entry[inode]
                    op = os.path.join(dirpath, entry)
                    ops.append((checksum, op))
            elif source_collection.get("from_operation"):
                raise NotImplementedError # TODO: make a test for this
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]
                source_celltype = source_collection.get("celltype", "bytes")
            elif source_collection.get("copied"):
                origin_collection = source_collection
                origin_collection_name = source_collection["name"]
                source_celltype = source_collection.get("celltype", "bytes")
                collection_dir = get_subdir("collections")
                source_collection_dir = os.path.join(collection_dir, source_collection_name)
                result_file = os.path.join(collection_dir, source_collection_name) + ".csv"
                result = load_csv(result_file)
                done_checksums = set()
                for entry, checksum in result.items():
                    if checksum in old_operation:
                        result_checksum = old_operation[checksum]
                        result_checksum_file = os.path.join(buffer_dir, result_checksum)
                        if os.path.exists(result_checksum_file):
                            noperation += 1
                            operation[checksum] = result_checksum
                            continue                
                    if checksum in done_checksums:
                        continue
                    noperation += 1                
                    done_checksums.add(checksum)
                    op = os.path.join(source_collection_dir, entry)
                    ops.append((checksum, op))
            else:
                raise AssertionError(source_collection)

            futures = []
            chunksize = 30
            last_operation_write = time.time()
            curr_ops = []
            assert source_celltype is not None
            if target_celltype is None:
                target_celltype = source_celltype
            for n in range(len(ops)):
                op = ops[n]
                curr_ops.append(op)
                last = (n == len(ops) - 1)
                cleanup = False
                if (n+1) % 1000 == 0:
                    if source_celltype != target_celltype:
                        cleanup = True
                if len(curr_ops) == chunksize or last: 
                    future = executor.submit(
                        do_operation, cmd, 
                        source_celltype, target_celltype,
                        curr_ops
                    )
                    futures.append(future)
                    curr_ops = []
                while len(futures) and (len(futures) == nproc or last or cleanup):
                    for future in futures:
                        if not future.done():
                            continue
                        result = future.result()
                        futures.remove(future)
                        for checksum, result_checksum in result:
                            if result_checksum is not None:
                                operation[checksum] = result_checksum
                                if len(operation) % 1000 == 0:
                                    report("Operated {}/{} files".format(
                                        len(operation), noperation
                                    ))

                                elapsed_time = time.time()
                                if elapsed_time - last_operation_write > 20:
                                    report("Save intermediate operation table to {}".format(operation_file)) 
                                    write_csv(operation, operation_file)
                                    last_operation_write = elapsed_time
                        break
                    else:
                        time.sleep(1)
                if cleanup:
                    executor.shutdown()
                    executor = concurrent.futures.ProcessPoolExecutor()
            assert not len(futures)
            write_csv(operation, operation_file)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "from_operation": True,
                "celltype": target_celltype,
                "origin": origin_collection_name
            }
                        
        elif name == "copy_collection":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]                    
            target_collection_name = action["collection"]
            assert target_collection_name not in collections, target_collection_name
            collection_dir = get_subdir("collections")
            target_collection_dir = os.path.join(collection_dir, target_collection_name)
            buffer_dir = get_subdir("buffers")
            interned = False
            can_hardlink = False
            if source_collection.get("default"):
                celltype = None
                origin_collection_name = default_collection_name
                origin_collection = collections[origin_collection_name]
                can_hardlink = cross_device
                if origin_collection.get("interned"):
                    interned = True
            elif source_collection.get("from_operation"):
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]
                can_hardlink = True
                interned = True
                celltype = source_collection["celltype"]
                if celltype == "bytes":
                    celltype = None
            elif source_collection.get("copied"):
                origin_collection_name = source_collection_name
                origin_collection = source_collection
                can_hardlink = True
                interned = True
                celltype = source_collection.get("celltype")
                if celltype == "bytes":
                    celltype = None
            else:
                raise AssertionError(source_collection)
            hardlink = action["hardlink"]
            if hardlink and not can_hardlink:
                msg = 'Action "copy_collection": source collection "{}" cannot be hardlinked'
                err(msg.format(source_collection_name))

            raw_filenames = {}
            origin_checksums = {}
            if origin_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(dirpath, entry)
            elif origin_collection.get("copied"):
                collection_dir = get_subdir("collections")
                origin_collection_dir = os.path.join(collection_dir, origin_collection["name"])
                result_file = os.path.join(collection_dir, origin_collection_name) + ".csv"
                result = load_csv(result_file)
                deepcell = {}
                for entry, checksum in result.items():
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(origin_collection_dir, entry)
            else:
                raise AssertionError(origin_collection)

            if source_collection.get("from_operation"):
                operation_dir = get_subdir("operations")
                operation_file = os.path.join(operation_dir, source_collection_name) + ".csv"
                operation = load_csv(operation_file)
                checksums = {entry: operation[checksum] for entry, checksum in origin_checksums.items()}
            else:
                checksums = origin_checksums

            if interned:
                filenames = {
                    entry:os.path.join(buffer_dir, checksum)
                    for entry, checksum in checksums.items()
                }
            else:
                filenames = raw_filenames

            result = {}
            regex = re.compile(action["source_file"])
            target_file_template = action["target_file"]
            to_copy = []
            count = 0

            for source_entry, checksum in checksums.items():
                if count > 0 and count % 50000 == 0:
                    report("Checking {}/{} files".format(
                        count, len(checksums)
                    ))      
                count += 1             
                filename = filenames[source_entry]
                unnamed_capturing_groups = regex.match(source_entry).groups()
                named_capturing_groups = regex.match(source_entry).groupdict()                
                target_entry = target_file_template.format(
                    *unnamed_capturing_groups,
                    **named_capturing_groups
                )
                result[target_entry] = checksum

                target_file = os.path.join(target_collection_dir, target_entry)
                if os.path.exists(target_file):
                    old_ino = os.stat(target_file).st_ino
                    if hardlink:
                        ino = os.stat(filename).st_ino
                        if old_ino == ino:
                            continue
                    buf_file = os.path.join(buffer_dir, checksum)
                    if os.path.exists(buf_file):
                        ino = os.stat(buf_file).st_ino
                        if old_ino == ino:
                            continue
                    os.remove(target_file)
                to_copy.append((filename, target_file))
            for n in range(len(to_copy)):
                filename, target_file = to_copy[n]
                if n > 0 and n % 20000 == 0:
                    term = "Hardlink" if hardlink else "Copy"
                    report("{} {}/{} files".format(
                        term, n, len(to_copy)
                    ))                    
                os.makedirs(os.path.dirname(target_file), exist_ok=True)
                if hardlink:
                    os.link(filename, target_file)
                else:
                    shutil.copyfile(filename, target_file)

            result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
            old_result = {}
            if os.path.exists(result_file):
                old_result = load_csv(result_file)
            for target_entry in old_result:
                if target_entry not in result:
                    target_file = os.path.join(target_collection_dir, target_entry)
                    if os.path.exists(target_file):
                        os.remove(target_file)
            write_csv(result, result_file)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "copied": True,
            }
            if celltype is not None:
                collections[target_collection_name]["celltype"] = celltype
        
        elif name == "build_download_index":
            source_file_or_key = "source_file"
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            if source_collection.get("default"):
                entries = inode_to_entry.values()
            elif source_collection.get("copied"):
                collection_dir = get_subdir("collections")
                result_file = os.path.join(collection_dir, source_collection_name) + ".csv"
                result = load_csv(result_file)
                entries = result.keys()
            elif source_collection.get("deepcell"):
                source_file_or_key = "source_key"
                deepcell_dir = get_subdir("deepcells")
                deepcell_file = os.path.join(deepcell_dir, target_collection_name) + ".json"
                deepcell = json.load(open(deepcell_file))
                entries = deepcell.keys()
            else:
                msg = 'Action "build_download_index": source collection "{}" must be default, copied or deepcell'
                err(msg.format(source_collection_name))
            
            download_index = {}
            urls = action["urls"]
            regex = re.compile(action[source_file_or_key])
            for entry in entries:
                unnamed_capturing_groups = regex.match(entry).groups()
                named_capturing_groups = regex.match(entry).groupdict()
                target_urls = []
                for urldict in urls:
                    if isinstance(urldict, str):
                        urldict = {"url": urldict}
                    target = {}
                    for attr in "compression", "celltype":
                        if attr in urldict:
                            target[attr] = urldict[attr]
                    target["url"] = urldict["url"].format(
                        *unnamed_capturing_groups,
                        **named_capturing_groups
                    )
                    target_urls.append(target)
                download_index[entry] = target_urls
            download_index_dir = get_subdir("download_indices")
            download_index_file = os.path.join(download_index_dir, source_collection_name) + ".json"
            report("Write download index to {}".format(download_index_file))
            write_json(download_index, download_index_file)
        
        elif name == "deepfolder":
            source_collection_name = action.get("source_collection", default_collection_name)
            source_collection = collections[source_collection_name]
            deepfolder_dir = get_subdir("deepfolders")
            deepfolder_file = os.path.join(deepfolder_dir, source_collection_name) + ".json"
            if os.path.exists(deepfolder_file):
                old_deepfolder = json.load(open(deepfolder_file))
                old_deepfolder_checksum = calculate_checksum(get_plain_buffer(old_deepfolder), hex=True)
                old_deepfolder_inode = str(os.stat(deepfolder_file).st_ino)
            else:
                old_deepfolder_checksum = None
            deepfolder = {}
            if source_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    deepfolder[entry] = checksum        
            elif source_collection.get("copied"):
                collection_dir = get_subdir("collections")
                result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
                result = load_csv(result_file)
                deepfolder = result
            else:
                msg = 'Action "deepfolder": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))

            deepfolder_checksum = calculate_checksum(get_plain_buffer(deepfolder), hex=True)
            print("Computed deepfolder for {}: deep buffer checksum {}".format(source_collection_name, deepfolder_checksum))
            if old_deepfolder_checksum is not None and deepfolder_checksum != old_deepfolder_checksum:
                remove_stale_buffer_hardlink(old_deepfolder_checksum, old_deepfolder_inode)
            report("Write deepfolder deep buffer to {}".format(deepfolder_file))
            write_json(deepfolder, deepfolder_file)
            intern_buffer(deepfolder_checksum, deepfolder_file, hardlink=True)
            write_deepcontent(deepfolder_checksum, deepfolder)
        
        elif name == "deepcell":
            source_collection_name = action.get("source_collection", default_collection_name)
            target_collection_name = action["collection"]
            assert target_collection_name not in collections
            source_collection = collections[source_collection_name]
            deepcell_dir = get_subdir("deepcells")
            deepcell_file = os.path.join(deepcell_dir, target_collection_name) + ".json"
            if os.path.exists(deepcell_file):
                old_deepcell = json.load(open(deepcell_file))
                old_deepcell_checksum = calculate_checksum(get_plain_buffer(old_deepcell), hex=True)
                old_deepcell_inode = str(os.stat(deepcell_file).st_ino)
            else:
                old_deepcell_checksum = None
            deepcell = {}
            if source_collection.get("default"):
                celltype = None
            elif source_collection.get("copied"):
                celltype = source_collection.get("celltype")
            elif source_collection.get("from_operation"):
                celltype = source_collection.get("celltype")
            else:
                celltype = None
            
            if celltype != "mixed":
                msg = 'Action "deepcell": source collection "{}" must have celltype "mixed"'
                err(msg.format(source_collection_name))

            regex = re.compile(action["source_file"])
            target_key_template = action["target_key"]

            if source_collection.get("copied"):
                raise NotImplementedError # TODO: a test for this
                collection_dir = get_subdir("collections")
                result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
                result = load_csv(result_file)
                deepcell0 = result
            elif source_collection.get("from_operation"):
                operation_dir = get_subdir("operations")
                operation_file = os.path.join(operation_dir, source_collection_name) + ".csv"
                operation = load_csv(operation_file)
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]

                if origin_collection.get("default"):
                    raise NotImplementedError # TODO: a test for this
                    origin_checksums = {}
                    deepcell0 = {}
                    for inode in inode_table:
                        checksum, _ = inode_table[inode]
                        entry = inode_to_entry[inode]
                        deepcell0[entry] = operation[checksum]
                elif origin_collection.get("copied"):
                    collection_dir = get_subdir("collections")
                    result_file = os.path.join(collection_dir, origin_collection_name) + ".csv"
                    result = load_csv(result_file)
                    deepcell0 = {}
                    for entry, checksum in result.items():
                        deepcell0[entry] = operation[checksum]
                else:
                    raise AssertionError(origin_collection)
            else:
                raise AssertionError(source_collection)

            deepcell = {}
            for entry, checksum in deepcell0.items():
                try:
                    unnamed_capturing_groups = regex.match(entry).groups()
                    named_capturing_groups = regex.match(entry).groupdict()
                except AttributeError:
                    raise ValueError(entry) from None
                target_entry = target_key_template.format(
                    *unnamed_capturing_groups,
                    **named_capturing_groups
                )
                deepcell[target_entry] = checksum

            deepcell_checksum = calculate_checksum(get_plain_buffer(deepcell), hex=True)
            print("Computed deepcell for {} -> {}: deep buffer checksum {}".format(source_collection_name, target_collection_name, deepcell_checksum))
            if old_deepcell_checksum is not None and deepcell_checksum != old_deepcell_checksum:
                remove_stale_buffer_hardlink(old_deepcell_checksum, old_deepcell_inode)
            report("Write deepcell deep buffer to {}".format(deepcell_file))
            write_json(deepcell, deepcell_file)
            intern_buffer(deepcell_checksum, deepcell_file, hardlink=True)
            write_deepcontent(deepcell_checksum, deepcell)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "origin": source_collection_name,
                "deepcell": True,
            }
finally:
    executor.shutdown()